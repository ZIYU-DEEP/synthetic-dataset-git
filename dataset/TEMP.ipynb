{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T13:28:05.644046Z",
     "start_time": "2020-08-14T13:28:05.640575Z"
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import scipy.io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T13:29:09.133383Z",
     "start_time": "2020-08-14T13:29:09.128857Z"
    }
   },
   "outputs": [],
   "source": [
    "# #########################################################################\n",
    "# 0. Base Loader\n",
    "# #########################################################################\n",
    "class BaseLoader(ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_set = None  # must be of type torch.utils.data.Dataset\n",
    "        self.test_set = None  # must be of type torch.utils.data.Dataset\n",
    "\n",
    "    @abstractmethod\n",
    "    def loaders(self,\n",
    "                batch_size: int,\n",
    "                shuffle_train=True,\n",
    "                shuffle_test=False,\n",
    "                num_workers: int = 0) -> (DataLoader, DataLoader):\n",
    "        \"\"\"Implement data loaders of type torch.utils.data.DataLoader for train_set and test_set.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T13:47:38.820656Z",
     "start_time": "2020-08-14T13:47:38.811777Z"
    }
   },
   "outputs": [],
   "source": [
    "# #########################################################################\n",
    "# 1. Dataset for Training\n",
    "# #########################################################################\n",
    "class SatimageDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root: str='../data/satimage-2.mat',\n",
    "                 label_abnormal: tuple=(),  # If unsupervised, do not specify\n",
    "                 train: bool=True,\n",
    "                 split: float=0.2,\n",
    "                 random_state: int=42):\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "        # Initialization\n",
    "        self.root = root\n",
    "        self.label_abnormal = label_abnormal\n",
    "        \n",
    "        # Load data\n",
    "        mat = scipy.io.loadmat(root)\n",
    "        \n",
    "        X = mat['X']\n",
    "        y = mat['y'].reshape(-1)           \n",
    "        \n",
    "        if not label_abnormal:\n",
    "            X = X[y == 0]\n",
    "            y = y[y == 0]\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=split,\n",
    "                                                            random_state=random_state,\n",
    "                                                            stratify=y)\n",
    "        \n",
    "        if train:\n",
    "            self.X = torch.tensor(X_train, dtype=torch.float32)\n",
    "            self.y = torch.tensor(y_train, dtype=torch.float32)\n",
    "        else:\n",
    "            self.X = torch.tensor(X_test, dtype=torch.float32)\n",
    "            self.y = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample, target = self.X[index], int(self.y[index])\n",
    "        return sample, target, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T13:47:39.316038Z",
     "start_time": "2020-08-14T13:47:39.302636Z"
    }
   },
   "outputs": [],
   "source": [
    "kk = SatimageDataset()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T13:47:45.308340Z",
     "start_time": "2020-08-14T13:47:45.297751Z"
    }
   },
   "source": [
    "Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T13:32:34.415748Z",
     "start_time": "2020-08-14T13:32:34.409367Z"
    }
   },
   "outputs": [],
   "source": [
    "# #########################################################################\n",
    "# 2. Loader for Training\n",
    "# #####################################uu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Tu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Teu,Te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = SatimageDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatimageLoader(BaseDataset):\n",
    "    def __init__(self,\n",
    "                 root: str='../data/satimage-2.mat',\n",
    "                 label_normal: tuple=(0,),\n",
    "                 label_abnormal: tuple=(),  # If unsupervised, do not specify\n",
    "                 ratio_abnormal: float=0.1,\n",
    "                 split: float=0.2,\n",
    "                 random_state: int=42):\n",
    "        super().__init__(root)\n",
    "\n",
    "        # Initialization\n",
    "        self.root = root\n",
    "        self.label_normal = label_normal\n",
    "        self.label_abnormal = label_abnormal\n",
    "        self.ratio_abnormal = ratio_abnormal\n",
    "        \n",
    "        # Load data\n",
    "        mat = scipy.io.loadmat(root)\n",
    "        X = mat['X']\n",
    "        y = mat['y'].reshape(-1)\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                            test_size=split,\n",
    "                                                            random_state=random_state,\n",
    "                                                            stratify=y)\n",
    "        \n",
    "        if train:\n",
    "            self.X = torch.tensor(X_train, dtype=torch.float32)\n",
    "            self.y = torch.tensor(y_train, dtype=torch.float32)\n",
    "        else:\n",
    "            self.X = torch.tensor(X_test, dtype=torch.float32)\n",
    "            self.y = torch.tensor(y_test, dtype=torch.float32)\n",
    "        \n",
    "\n",
    "        # Read in initial Full Set\n",
    "        # Add in download=True if you haven't downloaded yet\n",
    "        print('Loading dataset for you!')\n",
    "        train_set = CIFAR10Dataset(root=root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "        test_set = CIFAR10Dataset(root=root, train=False, transform=transforms.ToTensor(), download=True)\n",
    "        print('Almost loaded!')\n",
    "\n",
    "        # Get the labels for classes intended to use\n",
    "        y_train = np.array(train_set.targets)\n",
    "        y_test = np.array(test_set.targets)\n",
    "\n",
    "        # Get the indices for classes intended to use\n",
    "        train_idx = self.get_idx(y_train, label_normal, label_abnormal, ratio_abnormal, True)\n",
    "        test_idx = self.get_idx(y_test, label_normal, label_abnormal, ratio_abnormal, False)\n",
    "\n",
    "        # Get the subset\n",
    "        self.train_set = Subset(train_set, train_idx)\n",
    "        self.test_set = Subset(test_set, test_idx)\n",
    "\n",
    "\n",
    "    def loaders(self,\n",
    "                batch_size: int,\n",
    "                shuffle_train=True,\n",
    "                shuffle_test=False,\n",
    "                num_workers: int = 0) -> (DataLoader, DataLoader):\n",
    "        train_loader = DataLoader(dataset=self.train_set,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle_train,\n",
    "                                  num_workers=num_workers,\n",
    "                                  drop_last=True)\n",
    "        test_loader = DataLoader(dataset=self.test_set,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=shuffle_test,\n",
    "                                 num_workers=num_workers,\n",
    "                                 drop_last=False)\n",
    "        return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T13:13:29.467761Z",
     "start_time": "2020-08-14T13:13:26.088551Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title: cifar10_loader.py\n",
    "Description: The loader classes for the FashionMNIST datasets\n",
    "Author: Lek'Sai Ye, University of Chicago\n",
    "\"\"\"\n",
    "\n",
    "from PIL import Image\n",
    "from abc import ABC, abstractmethod\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# #########################################################################\n",
    "# 1. Base Dataset\n",
    "# #########################################################################\n",
    "class BaseDataset(ABC):\n",
    "    def __init__(self, root: str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = root\n",
    "        self.label_normal = ()\n",
    "        self.label_abnormal = ()\n",
    "        self.train_set = None\n",
    "        self.test_set = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def loaders(self,\n",
    "                batch_size: int,\n",
    "                shuffle_train=True,\n",
    "                shuffle_test=False,\n",
    "                num_workers: int = 0):\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "# #########################################################################\n",
    "# 2. CIFAR10 Dataset\n",
    "# #########################################################################\n",
    "class CIFAR10Dataset(CIFAR10):\n",
    "    \"\"\"\n",
    "    Add an index to get item.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        img = Image.fromarray(img)\n",
    "        transform = transforms.ToTensor()\n",
    "        img = transform(img)\n",
    "        return img, int(target), index\n",
    "\n",
    "\n",
    "# #########################################################################\n",
    "# 3. CIFAR10 Loader for Training\n",
    "# #########################################################################\n",
    "class CIFAR10Loader(BaseDataset):\n",
    "    def __init__(self,\n",
    "                 root: str='/net/leksai/data/CIFAR10',\n",
    "                 label_normal: tuple=(0,),\n",
    "                 label_abnormal: tuple=(),  # If unsupervised, do not specify\n",
    "                 ratio_abnormal: float=0.1):\n",
    "        super().__init__(root)\n",
    "\n",
    "        # Initialization\n",
    "        self.root = root\n",
    "        self.label_normal = label_normal\n",
    "        self.label_abnormal = label_abnormal\n",
    "        self.ratio_abnormal = ratio_abnormal\n",
    "\n",
    "        # Read in initial Full Set\n",
    "        # Add in download=True if you haven't downloaded yet\n",
    "        print('Loading dataset for you!')\n",
    "        train_set = CIFAR10Dataset(root=root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "        test_set = CIFAR10Dataset(root=root, train=False, transform=transforms.ToTensor(), download=True)\n",
    "        print('Almost loaded!')\n",
    "\n",
    "        # Get the labels for classes intended to use\n",
    "        y_train = np.array(train_set.targets)\n",
    "        y_test = np.array(test_set.targets)\n",
    "\n",
    "        # Get the indices for classes intended to use\n",
    "        train_idx = self.get_idx(y_train, label_normal, label_abnormal, ratio_abnormal, True)\n",
    "        test_idx = self.get_idx(y_test, label_normal, label_abnormal, ratio_abnormal, False)\n",
    "\n",
    "        # Get the subset\n",
    "        self.train_set = Subset(train_set, train_idx)\n",
    "        self.test_set = Subset(test_set, test_idx)\n",
    "\n",
    "    def get_idx(self, y, label_normal, label_abnormal, ratio_abnormal, train):\n",
    "        \"\"\"\n",
    "        Creat a numpy list of indices of label_ in labels.\n",
    "        Inputs:\n",
    "            y (np.array): dataset.targets.cpu().data.numpy()\n",
    "            label_normal (tuple): e.g. (0,)\n",
    "            label_abnormal (tuple): e.g. (1,)\n",
    "            ratio_abnormal (float): e.g. 0.1\n",
    "            train (bool): True / False\n",
    "        \"\"\"\n",
    "        idx_normal = np.argwhere(np.isin(y, label_normal)).flatten()\n",
    "\n",
    "        if label_abnormal:\n",
    "            idx_abnormal = np.argwhere(np.isin(y, label_abnormal)).flatten()\n",
    "            np.random.shuffle(idx_abnormal)\n",
    "            if train:\n",
    "                idx_abnormal = idx_abnormal[:int(len(idx_abnormal) * ratio_abnormal)]\n",
    "            idx_all = np.hstack((idx_normal, idx_abnormal))\n",
    "        else:\n",
    "            idx_all = idx_normal\n",
    "        return idx_all\n",
    "\n",
    "    def loaders(self,\n",
    "                batch_size: int,\n",
    "                shuffle_train=True,\n",
    "                shuffle_test=False,\n",
    "                num_workers: int = 0) -> (DataLoader, DataLoader):\n",
    "        train_loader = DataLoader(dataset=self.train_set,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle_train,\n",
    "                                  num_workers=num_workers,\n",
    "                                  drop_last=True)\n",
    "        test_loader = DataLoader(dataset=self.test_set,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=shuffle_test,\n",
    "                                 num_workers=num_workers,\n",
    "                                 drop_last=False)\n",
    "        return train_loader, test_loader\n",
    "\n",
    "\n",
    "# #########################################################################\n",
    "# 4. CIFAR10 Loader for Evaluation\n",
    "# #########################################################################\n",
    "class CIFAR10LoaderEval(BaseDataset):\n",
    "    def __init__(self,\n",
    "                 root: str='/net/leksai/data/CIFAR10',\n",
    "                 label: tuple=(),\n",
    "                 test_eval: bool=False):\n",
    "        super().__init__(root)\n",
    "\n",
    "        # Initialization\n",
    "        self.root = root\n",
    "        self.label = label\n",
    "\n",
    "        # Read in initial Full Set\n",
    "        # Add in download=True if you haven't downloaded yet\n",
    "        train_set = CIFAR10Dataset(root=root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "        test_set = CIFAR10Dataset(root=root, train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "        # Get the labels for classes intended to use\n",
    "        y_train = np.array(train_set.targets)\n",
    "        y_test = np.array(test_set.targets)\n",
    "\n",
    "        # Get the indices for classes intended to use\n",
    "        train_idx = self.get_idx(y_train, label)\n",
    "        test_idx = self.get_idx(y_test, label)\n",
    "\n",
    "        # Get the subset\n",
    "        train_set = Subset(train_set, train_idx)\n",
    "        test_set = Subset(test_set, test_idx)\n",
    "        if test_eval:\n",
    "            self.all_set = test_set\n",
    "        else:\n",
    "            self.all_set = ConcatDataset((train_set, test_set))\n",
    "\n",
    "    def get_idx(self, y, label):\n",
    "        \"\"\"\n",
    "        Creat a numpy list of indices of label_ in labels.\n",
    "        Inputs:\n",
    "            y (np.array): dataset.targets.cpu().data.numpy()\n",
    "            label (tuple): e.g. (0,)\n",
    "        \"\"\"\n",
    "        return np.argwhere(np.isin(y, label)).flatten()\n",
    "\n",
    "    def loaders(self,\n",
    "                batch_size: int,\n",
    "                shuffle=False,\n",
    "                num_workers: int = 0):\n",
    "        all_loader = DataLoader(dataset=self.all_set,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=shuffle,\n",
    "                                num_workers=num_workers,\n",
    "                                drop_last=False)\n",
    "        return all_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
